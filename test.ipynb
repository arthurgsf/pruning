{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 10:41:21.388269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-01 10:41:21.430693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-01 10:41:21.431216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "IMG_SHAPE = (256, 256)\n",
    "N_CHANNELS = 1\n",
    "INPUT_SHAPE = IMG_SHAPE + (N_CHANNELS,)\n",
    "N_CLASSES = 1\n",
    "OUTPUT_SHAPE = IMG_SHAPE + (N_CLASSES,)\n",
    "BATCH_SIZE = 20\n",
    "DATASET_PATH = f\"{os.path.expanduser('~')}/Datasets/segthor_extracted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "dice_score = sm.metrics.FScore(threshold=0.5)\n",
    "precision_score = tf.keras.metrics.Precision(thresholds=0.5)\n",
    "recall_score = tf.keras.metrics.Recall(thresholds=0.5)\n",
    "iou_score = sm.metrics.IOUScore(threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fc3638c69e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import EfficientUnet\n",
    "model = EfficientUnet(input_shape = INPUT_SHAPE)\n",
    "model.load_weights(f\"records/{model.name}/checkpoint/{model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_07', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_06', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_10', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_08', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_11', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_05', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_09', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_03', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_01', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_04', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_02', '/home/arthur_guilherme/Datasets/segthor_extracted/test/Patient_12']\n",
      "9/9 [==============================] - 2s 127ms/step\n",
      "10/10 [==============================] - 1s 82ms/step\n",
      "9/9 [==============================] - 1s 89ms/step\n",
      "9/9 [==============================] - 1s 108ms/step\n",
      "9/9 [==============================] - 1s 83ms/step\n",
      "15/15 [==============================] - 1s 84ms/step\n",
      "8/8 [==============================] - 1s 120ms/step\n",
      "8/8 [==============================] - 1s 101ms/step\n",
      "12/12 [==============================] - 1s 95ms/step\n",
      "8/8 [==============================] - 1s 133ms/step\n",
      "13/13 [==============================] - 1s 91ms/step\n",
      "9/9 [==============================] - 1s 80ms/step\n",
      "dice : 0.7283074259757996\n",
      "precision : 0.6025879979133606\n",
      "recall : 0.9349696040153503\n",
      "iou_score : 0.5783645510673523\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import preprocessing\n",
    "from glob import glob\n",
    "from generators import PerPatientSliceGenerator\n",
    "\n",
    "preprocessing_pipeline = preprocessing.Pipeline([\n",
    "    preprocessing.windowing(-500, 60),\n",
    "    preprocessing.norm, \n",
    "    preprocessing.resize(IMG_SHAPE),\n",
    "    preprocessing.expand_dims\n",
    "])\n",
    "\n",
    "metrics = {\n",
    "    \"dice\":[],\n",
    "    \"precision\":[],\n",
    "    \"recall\":[],\n",
    "    \"iou_score\":[],\n",
    "}\n",
    "\n",
    "patients_test = glob(DATASET_PATH + '/test/*')\n",
    "\n",
    "for patient in patients_test:\n",
    "    patient_dataset = tf.data.Dataset.from_generator(\n",
    "        PerPatientSliceGenerator(patient, preprocessing_pipeline),\n",
    "        output_signature=\n",
    "        (\n",
    "            tf.TensorSpec(shape=INPUT_SHAPE, dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=OUTPUT_SHAPE, dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    volume_true = []\n",
    "    for (x, y) in patient_dataset:\n",
    "        volume_true.append(y)\n",
    "    volume_true = np.squeeze(np.array(volume_true))\n",
    "    volume_pred = model.predict(patient_dataset.batch(20))\n",
    "    volume_pred = np.round(volume_pred)\n",
    "    volume_pred = np.squeeze(volume_pred)\n",
    "\n",
    "    metrics[\"dice\"].append(dice_score(volume_true, volume_pred))\n",
    "    metrics[\"precision\"].append(precision_score(volume_true, volume_pred))\n",
    "    metrics[\"recall\"].append(recall_score(volume_true, volume_pred))\n",
    "    metrics[\"iou_score\"].append(iou_score(volume_true, volume_pred))\n",
    "    \n",
    "for k in metrics.keys():\n",
    "    print(f\"{k} : {np.mean(metrics[k])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 82ms/step\n",
      "10/10 [==============================] - 1s 80ms/step\n",
      "9/9 [==============================] - 1s 75ms/step\n",
      "9/9 [==============================] - 1s 78ms/step\n",
      "9/9 [==============================] - 1s 82ms/step\n",
      "15/15 [==============================] - 1s 74ms/step\n",
      "8/8 [==============================] - 1s 80ms/step\n",
      "8/8 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "8/8 [==============================] - 1s 82ms/step\n",
      "13/13 [==============================] - 1s 75ms/step\n",
      "9/9 [==============================] - 1s 78ms/step\n",
      "dice : 0.8052718043327332\n",
      "precision : 0.6248044967651367\n",
      "recall : 0.9329025745391846\n",
      "iou_score : 0.6774531006813049\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import preprocessing\n",
    "import postprocessing\n",
    "from glob import glob\n",
    "from generators import PerPatientSliceGenerator\n",
    "\n",
    "preprocessing_pipeline = preprocessing.Pipeline([\n",
    "    preprocessing.windowing(-500, 60),\n",
    "    preprocessing.norm, \n",
    "    preprocessing.resize(IMG_SHAPE),\n",
    "    preprocessing.expand_dims\n",
    "])\n",
    "\n",
    "metrics = {\n",
    "    \"dice\":[],\n",
    "    \"precision\":[],\n",
    "    \"recall\":[],\n",
    "    \"iou_score\":[],\n",
    "}\n",
    "\n",
    "patients_test = glob(DATASET_PATH + '/test/*')\n",
    "\n",
    "for patient in patients_test:\n",
    "    patient_dataset = tf.data.Dataset.from_generator(\n",
    "        PerPatientSliceGenerator(patient, preprocessing_pipeline),\n",
    "        output_signature=\n",
    "        (\n",
    "            tf.TensorSpec(shape=INPUT_SHAPE, dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=OUTPUT_SHAPE, dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    volume_true = []\n",
    "    for (x, y) in patient_dataset:\n",
    "        volume_true.append(y)\n",
    "    volume_true = np.squeeze(np.array(volume_true))\n",
    "    volume_pred = model.predict(patient_dataset.batch(20))\n",
    "    volume_pred = np.round(volume_pred)\n",
    "    volume_pred = np.squeeze(volume_pred)\n",
    "    volume_pred = postprocessing.biggest_3D_object(volume_pred)\n",
    "\n",
    "    metrics[\"dice\"].append(dice_score(volume_true, volume_pred))\n",
    "    metrics[\"precision\"].append(precision_score(volume_true, volume_pred))\n",
    "    metrics[\"recall\"].append(recall_score(volume_true, volume_pred))\n",
    "    metrics[\"iou_score\"].append(iou_score(volume_true, volume_pred))\n",
    "\n",
    "for k in metrics.keys():\n",
    "    print(f\"{k} : {np.mean(metrics[k])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arthur_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
